{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим нужные библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plt, mpl\n",
    "import seaborn as sns\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "import math\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Чистка отклонений\n",
    "import scipy.stats as stats\n",
    "# Уберем warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Тематическое моделирование\n",
    "import gensim\n",
    "\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgbm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dcb340",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f2017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_json('train.json')\n",
    "test_df = pd.read_json('test.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Чистка широты и долготы\n",
    "train_df[\"longitude\"]=train_df[\"longitude\"].apply(lambda x:-73.75 if x>=-73.75 else x)\n",
    "test_df[\"longitude\"]=test_df[\"longitude\"].apply(lambda x:-74.05 if x<=-74.05 else x)\n",
    "train_df[\"latitude\"]=train_df[\"latitude\"].apply(lambda x:40.4 if x<=40.4 else x)\n",
    "test_df[\"latitude\"]=test_df[\"latitude\"].apply(lambda x:40.9 if x>=40.9 else x)\n",
    "\n",
    "mean_price = int(train_df['price'].mean())\n",
    "test_df.loc[test_df['price']<200,'price'] = mean_price\n",
    "train_df.loc[train_df['price']<200,'price'] = mean_price\n",
    "# Метапризнак предсказ цены на основе координат\n",
    "#X_train_coor = train_df[['latitude', 'longitude']]\n",
    "#X_test_coor = test_df[['latitude', 'longitude']]\n",
    "\n",
    "#y_train_price = train_df.price\n",
    "#y_test_price = test_df.price\n",
    "\n",
    "#neigh_train = KNeighborsRegressor(n_neighbors=2)\n",
    "#neigh_train.fit(X_train_coor, y_train_price)\n",
    "\n",
    "#neigh_test = KNeighborsRegressor(n_neighbors=2)\n",
    "#neigh_test.fit(X_test_coor, y_test_price)\n",
    "\n",
    "#train_df['price_comparison'] = train_df['price'] /  neigh_train.predict(X_train_coor)\n",
    "#test_df['price_comparison'] = test_df['price'] /  neigh_test.predict(X_test_coor)\n",
    "\n",
    "\n",
    "# Функция расчета расстояния от манхэттэна\n",
    "def distance_pair(lat1, lon1, lat2, lon2):\n",
    "    # Перевод в радианы\n",
    "    p = 0.017453292519943295     #Pi/180\n",
    "    haver_formula = 0.5 - np.cos((lat2 - lat1) * p)/2 +  np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    d_2_point = 6367 *2 * np.arcsin(np.sqrt(haver_formula)) \n",
    "    return d_2_point\n",
    "distance_pairs = np.vectorize(distance_pair)\n",
    "\n",
    "# Расстояние от финансового центра\n",
    "train_df['distance_manh'] = distance_pairs(train_df.latitude, train_df.longitude, 40.70858733058446, -74.01098113951349)\n",
    "test_df['distance_manh'] = distance_pairs(test_df.latitude, test_df.longitude, 40.70858733058446, -74.01098113951349)\n",
    "\n",
    "\n",
    "# Расстояние от центр парка\n",
    "train_df['distance_central_park'] = distance_pairs(train_df.latitude, train_df.longitude, 40.77897128595648, -73.96656147253005)\n",
    "test_df['distance_central_park'] = distance_pairs(test_df.latitude, test_df.longitude, 40.77897128595648, -73.96656147253005)\n",
    "\n",
    "# Расстояние от  Бронкс\n",
    "train_df['bronx'] = distance_pairs(train_df.latitude, train_df.longitude, 40.84928609864644, -73.88844362698887)\n",
    "test_df['bronx'] = distance_pairs(test_df.latitude, test_df.longitude, 40.84928609864644, -73.88844362698887)\n",
    "\n",
    "# Расстояние от Район Queens\n",
    "\n",
    "train_df['queens'] = distance_pairs(train_df.latitude, train_df.longitude, 40.747803602867066, -73.90170458998642)\n",
    "test_df['queens'] = distance_pairs(test_df.latitude, test_df.longitude, 40.747803602867066, -73.90170458998642)\n",
    "\n",
    "# Расстояние от Район Brooklin кладбище\n",
    "\n",
    "train_df['brooklin_cometery'] = distance_pairs(train_df.latitude, train_df.longitude, 40.68822862016868, -73.87615328604328)\n",
    "test_df['brooklin_cometery'] = distance_pairs(test_df.latitude, test_df.longitude, 40.68822862016868, -73.87615328604328)\n",
    "\n",
    "# Расстояние от Район Brooklin\n",
    "train_df['brooklin'] = distance_pairs(train_df.latitude, train_df.longitude, 40.62506864788317, -73.96786745767038)\n",
    "test_df['brooklin'] = distance_pairs(test_df.latitude, test_df.longitude, 40.62506864788317, -73.96786745767038)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Кластеризация координат\n",
    "geo = ['latitude', 'longitude']\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# Метки кластеров внесем в переменную clusters\n",
    "train_df['clusters'] = kmeans.fit(train_df[geo]).labels_\n",
    "test_df['clusters'] = kmeans.fit(test_df[geo]).labels_\n",
    "\n",
    "\n",
    "#test_df = test.copy()\n",
    "\n",
    "\n",
    "train_df['num_photos'] = train_df['photos'].apply(len)\n",
    "test_df['num_photos'] = test_df['photos'].apply(len)\n",
    "\n",
    "# Число признаков\n",
    "train_df['num_features'] = train_df['features'].apply(len)\n",
    "test_df['num_features'] = test_df['features'].apply(len)\n",
    "\n",
    "# Количество слов\n",
    "train_df['num_description_words'] = train_df['description'].apply(lambda x: len(x.split(\" \")))\n",
    "test_df['num_description_words'] = test_df['description'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "\n",
    "# Даты\n",
    "train_df['created'] = pd.to_datetime(train_df['created'])\n",
    "test_df['created'] = pd.to_datetime(test_df['created'])\n",
    "\n",
    "# Извлекаем время\n",
    "train_df['created_year'] = train_df['created'].dt.year\n",
    "test_df['created_year'] = test_df['created'].dt.year\n",
    "train_df['created_month'] = train_df['created'].dt.month\n",
    "test_df['created_month'] = test_df['created'].dt.month\n",
    "train_df['created_day'] = train_df['created'].dt.day\n",
    "test_df['created_day'] = test_df['created'].dt.day\n",
    "train_df['created_hour'] = train_df['created'].dt.hour\n",
    "test_df['created_hour'] = test_df['created'].dt.hour\n",
    "\n",
    "\n",
    "\n",
    "# Фичи по ванной и спальням\n",
    "# Разл в комнатах\n",
    "train_df['room_difference'] = train_df['bedrooms'] - train_df['bathrooms']\n",
    "test_df['room_difference'] = test_df['bedrooms'] - test_df['bathrooms']\n",
    "# Общее количество\n",
    "train_df['total_rooms'] = train_df['bedrooms'] + train_df['bathrooms']\n",
    "test_df['total_rooms'] = test_df['bedrooms'] + test_df['bathrooms']\n",
    "# Цена 1 добавляется чтобы исключить деление на ноль\n",
    "train_df['price_per_room'] = train_df['price'] / (train_df['total_rooms'] + 1)\n",
    "test_df['price_per_room'] = test_df['price'] / (test_df['total_rooms'] + 1)\n",
    "# Цена за спальню\n",
    "train_df['price_per_bedroom'] =train_df['price']/(train_df['bedrooms'] + 1)\n",
    "test_df['price_per_bedroom'] =test_df['price']/(test_df['bedrooms'] +1)\n",
    "\n",
    "# Цена за ванную\n",
    "train_df['price_per_bathroom'] =train_df['price']/(train_df['bathrooms'] +1)\n",
    "test_df['price_per_bathroom'] =test_df['price']/(test_df['bathrooms'] +1)\n",
    "\n",
    "# Близость к метро\n",
    "subway = pd.read_csv('NYC_Transit_Subway_Entrance_And_Exit_Data.csv') # сторонние данные\n",
    "subway = subway[['Station Name', 'Station Latitude', 'Station Longitude']]\n",
    "subway = subway.groupby(['Station Name']).mean().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Функция минимального расстояния от метро\n",
    "def get_subway_distance(location):\n",
    "    distances = distance_pairs(location[0], location[1], subway['Station Latitude'], subway['Station Longitude'])    \n",
    "    return min(distances)\n",
    "\n",
    "# Расчет минимальной дистанции до метро\n",
    "train_df['location'] = train_df[['latitude', 'longitude']].values.tolist()\n",
    "test_df['location'] = test_df[['latitude', 'longitude']].values.tolist()\n",
    "\n",
    "train_df['subway_distance'] = train_df['location'].apply(get_subway_distance)\n",
    "test_df['subway_distance'] = test_df['location'].apply(get_subway_distance)\n",
    "\n",
    "\n",
    "# Плотность по координатам https://www.kaggle.com/code/maheshak04/rh004\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "\n",
    "\n",
    "# Фичи отсюда https://www.kaggle.com/code/slavik0505/kernel43aae13b62\n",
    "train_df[\"listing_id1\"] = train_df[\"listing_id\"] - 68119576.0\n",
    "test_df[\"listing_id1\"] =  test_df[\"listing_id\"] - 68119576.0\n",
    "\n",
    "train_df[\"total_days\"] =   (train_df[\"created_month\"] -4.0)*30 + train_df[\"created_day\"] +  train_df[\"created_hour\"] /25.0\n",
    "test_df[\"total_days\"] =(test_df[\"created_month\"] -4.0)*30 + test_df[\"created_day\"] +  test_df[\"created_hour\"] /25.0        \n",
    "train_df[\"diff_rank\"]= train_df[\"total_days\"]/train_df[\"listing_id1\"]\n",
    "test_df[\"diff_rank\"]= test_df[\"total_days\"]/test_df[\"listing_id1\"]\n",
    "\n",
    "# Фича из фото нужно скачать торрент файл\n",
    "df_time = pd.read_csv('listing_image_time.csv')\n",
    "train_df = train_df.merge(df_time, how='left', on='listing_id', suffixes=('', '_remove'))\n",
    "test_df = test_df.merge(df_time, how='left', on='listing_id', suffixes=('', '_remove'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac75a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фичи отсюда https://www.kaggle.com/code/slavik0505/kernel43aae13b62\n",
    "train_df[\"price0\"] = (train_df[\"price\"]%10).astype(int)\n",
    "test_df[\"price0\"] = (test_df[\"price\"]%10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a3f0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фичи отсюда https://www.kaggle.com/code/slavik0505/kernel43aae13b62\n",
    "train_df[\"price_latitude\"] = (train_df[\"price\"])/ (train_df[\"latitude\"]+1.0)\n",
    "test_df[\"price_latitude\"] =  (test_df[\"price\"])/ (test_df[\"latitude\"]+1.0)\n",
    "\n",
    "train_df[\"price_longtitude\"] = (train_df[\"price\"])/ (train_df[\"longitude\"]-1.0)\n",
    "test_df[\"price_longtitude\"] =  (test_df[\"price\"])/ (test_df[\"longitude\"]-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8960d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use  = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea97fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use.extend(['num_photos', \n",
    "                        'num_features', \n",
    "                        'num_description_words',\n",
    "                        'created_year', \n",
    "                        'created_month', \n",
    "                        'created_day', \n",
    "                        'listing_id', \n",
    "                        'created_hour', \n",
    "                        'clusters',\n",
    "                        'distance_manh',\n",
    "                        'distance_central_park',\n",
    "                        'room_difference',\n",
    "                        'total_rooms',\n",
    "                        'price_per_room',\n",
    "                         'bronx',\n",
    "                        'distance_central_park',\n",
    "                        'queens', \n",
    "                        'brooklin_cometery', \n",
    "                        'brooklin', \n",
    "                        'clusters',\n",
    "                        'num_description_words', \n",
    "                        'created_year',\n",
    "                        'created_month', \n",
    "                        'created_day', \n",
    "                        'created_hour', \n",
    "                        'room_difference',\n",
    "                        'total_rooms', \n",
    "                        'subway_distance', \n",
    "                        \"density\", \n",
    "                        \"price_latitude\", \n",
    "                        \"price_longtitude\", \n",
    "                        'total_days', \n",
    "                        'diff_rank', \n",
    "                        'time_stamp', \n",
    "                        'price0', \n",
    "                        'price_per_bedroom', \n",
    "                        'price_per_bathroom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85334f4e",
   "metadata": {},
   "source": [
    "# FE Кодирование менеджеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f003ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "start_values = [0,0,0]\n",
    "\n",
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]= start_values.copy()\n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]= start_values.copy()\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed3ca62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция обучения\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=2017, num_rounds=2000):\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'verbose': 0,\n",
    "        'num_leaves': 54, 'learning_rate': 0.05, 'max_depth': -1,\n",
    "        'num_class': 3, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6, \n",
    "        'reg_alpha': 1, 'reg_lambda': 0.001, 'metric': 'multi_logloss',\n",
    "        'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed_val)\n",
    "    for dev_index, val_index in kf.split(train_X, train_y):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        train_set = lgbm.Dataset(dev_X, dev_y, silent=True)\n",
    "        val_set = lgbm.Dataset(val_X, val_y, silent=True)\n",
    "        \n",
    "        model = lgbm.train(params, train_set=train_set, num_boost_round=num_rounds, valid_sets=val_set,\n",
    "                        early_stopping_rounds=50, evals_result=None, verbose_eval=100, learning_rates=None, \n",
    "                        callbacks=None)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration = model.best_iteration)\n",
    "    return pred_test_y, lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a659b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a52fd38eaed5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunXGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mout_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mout_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"high\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"medium\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"low\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-1334d8a066cf>\u001b[0m in \u001b[0;36mrunXGB\u001b[1;34m(train_X, train_y, test_X, test_y, feature_names, seed_val, num_rounds)\u001b[0m\n\u001b[0;32m      9\u001b[0m         'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdev_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdev_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "# label encoding\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27da277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.53082\n",
      "[200]\tvalid_0's multi_logloss: 0.521268\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's multi_logloss: 0.520094\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.518461\n",
      "[200]\tvalid_0's multi_logloss: 0.507398\n",
      "[300]\tvalid_0's multi_logloss: 0.504929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[295]\tvalid_0's multi_logloss: 0.50485\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.528969\n",
      "[200]\tvalid_0's multi_logloss: 0.520569\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid_0's multi_logloss: 0.519735\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.523141\n",
      "[200]\tvalid_0's multi_logloss: 0.512999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's multi_logloss: 0.511208\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid_0's multi_logloss: 0.51117\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.523992\n",
      "[200]\tvalid_0's multi_logloss: 0.512462\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's multi_logloss: 0.509452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's multi_logloss: 0.509473\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's multi_logloss: 0.509246\n"
     ]
    }
   ],
   "source": [
    "print('start training')\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=2000)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c42044c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"lgbm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c86e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
